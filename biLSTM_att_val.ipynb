{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text as kpt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Constants\n",
    "# ----------------------------------------------\n",
    "num_units = 300\n",
    "batch_size = 50\n",
    "embedding_dim = 300\n",
    "story_length = 4\n",
    "scale = 1.0\n",
    "# num_vocab = 20 # 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 10)\n",
      "7432\n",
      "11226\n",
      "(1871, 6, 18)\n",
      "(0, 18)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# Preprocess the sentences (Modified from Hong's code)\n",
    "# ----------------------------------------------\n",
    "##### Load data #####\n",
    "# TRAIN_URL = 'train_stories.csv'\n",
    "TRAIN_URL = 'eval_stories.csv'\n",
    "\n",
    "# InputStoryid,InputSentence1,InputSentence2,InputSentence3,InputSentence4,RandomFifthSentenceQuiz1,RandomFifthSentenceQuiz2,AnswerRightEnding\n",
    "df_train_sep = pd.read_csv(TRAIN_URL).iloc[:, 1:]\n",
    "\n",
    "df_train_sep['correct'] = df_train_sep[['InputSentence1','InputSentence2','InputSentence3',\\\n",
    "                                        'InputSentence4','RandomFifthSentenceQuiz1','RandomFifthSentenceQuiz2']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "df_train_sep['right_endings'] = df_train_sep['RandomFifthSentenceQuiz1']\n",
    "df_train_sep['right_endings'].update(df_train_sep.loc[df_train_sep['AnswerRightEnding'] == 2,'RandomFifthSentenceQuiz2'])\n",
    "\n",
    "df_train_sep['wrong_endings'] = df_train_sep['RandomFifthSentenceQuiz2']\n",
    "df_train_sep['wrong_endings'].update(df_train_sep.loc[df_train_sep['AnswerRightEnding'] == 2,'RandomFifthSentenceQuiz1'])\n",
    "\n",
    "print(df_train_sep.shape)\n",
    "\n",
    "\n",
    "##### Prepare dictionary #####\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenizer = kpt.Tokenizer(oov_token='<UNK>', num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train_sep['correct'])\n",
    "tokenizer.word_index = {w:i for w,i in tokenizer.word_index.items() if i < VOCAB_SIZE}\n",
    "\n",
    "vocab_dict = tokenizer.word_index\n",
    "print(len(vocab_dict)+1)\n",
    "\n",
    "##### Prepare for training set #####\n",
    "# X_train = pd.concat([df_train_sep['sentence1'], df_train_sep['sentence2'],\\\n",
    "# \tdf_train_sep['sentence3'], df_train_sep['sentence4'], df_train_sep['sentence5']])\n",
    "\n",
    "X_train = pd.concat([df_train_sep['InputSentence1'], df_train_sep['InputSentence2'],\\\n",
    "\tdf_train_sep['InputSentence3'], df_train_sep['InputSentence4'], \\\n",
    "                     df_train_sep['RandomFifthSentenceQuiz1'], df_train_sep['RandomFifthSentenceQuiz2']])\n",
    "\n",
    "print(len(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "seq_len = max(len(s) for s in X_train)\n",
    "n_correct = df_train_sep.shape[0]\n",
    "# n_samples = n_correct * 2\n",
    "\n",
    "X_train = np.zeros((n_correct,6,seq_len),dtype=np.int32)\n",
    "\n",
    "for i in range(1,5):\n",
    "\tX_train_temp = tokenizer.texts_to_sequences(df_train_sep[\"InputSentence\"+str(i)])\n",
    "\n",
    "\tX_train_temp = pad_sequences(X_train_temp, maxlen=seq_len, padding='post')\n",
    "\tX_train[:,i-1,:] = X_train_temp\n",
    "\n",
    "\n",
    "X_train_temp = tokenizer.texts_to_sequences(df_train_sep[\"right_endings\"])\n",
    "X_train_temp = pad_sequences(X_train_temp, maxlen=seq_len, padding='post')\n",
    "X_train[:,4,:] = X_train_temp\n",
    "\n",
    "X_train_temp = tokenizer.texts_to_sequences(df_train_sep[\"wrong_endings\"])\n",
    "X_train_temp = pad_sequences(X_train_temp, maxlen=seq_len, padding='post')\n",
    "X_train[:,5,:] = X_train_temp\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[n_correct:,4,:].shape)\n",
    "\n",
    "X_ori = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(20000, 300) 7105\n"
     ]
    }
   ],
   "source": [
    "##### load embeddings #####\n",
    "embedding_dim = 300\n",
    "GLOVE_URL = 'glove.6B.300d.txt'\n",
    "\n",
    "embeddings = {}\n",
    "with open(GLOVE_URL, 'r', encoding='utf-8') as f:\n",
    "\tfor line in f:\n",
    "\t\tvalues = line.split()\n",
    "\t\tw = values[0]\n",
    "\t\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\t\tembeddings[w] = coefs\n",
    "\n",
    "\n",
    "print(len(embeddings))\n",
    "\n",
    "embedding_matrix = np.random.uniform(-1, 1, size=(VOCAB_SIZE, embedding_dim))\n",
    "num_loaded = 0\n",
    "for w, i in vocab_dict.items():\n",
    "\tif w in embeddings and i < VOCAB_SIZE:\n",
    "\t\tembedding_matrix[i] = embeddings[w]\n",
    "\t\tnum_loaded += 1\n",
    "\n",
    "print(embedding_matrix.shape, num_loaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 300, 300) (18, 300, 300)\n",
      "(18, 300, 300)\n",
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# Set up the graph\n",
    "# ----------------------------------------------\n",
    "tf.reset_default_graph()\n",
    "\n",
    "##### embed sentences #####\n",
    "lstm_forward1 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "\t\t\t\t\t\t\t   initializer=tf.contrib.layers.xavier_initializer())\n",
    "lstm_backward1 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "\t\t\t\t\t\t\t   initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# should use GloVe to embed words first (not finished!)\n",
    "# same story are close to each other\n",
    "\n",
    "embedding = tf.placeholder('float32', [batch_size*(story_length+2), seq_len, embedding_dim])\n",
    "\n",
    "x = tf.unstack(embedding, seq_len, 1)\n",
    "\n",
    "state_forward1 = lstm_forward1.zero_state(batch_size*(story_length+2), 'float')\n",
    "state_backward1 = lstm_backward1.zero_state(batch_size*(story_length+2), 'float')\n",
    "\n",
    "states_forward1 = []\n",
    "states_backward1 = []\n",
    "for t in range(seq_len):\n",
    "\n",
    "\tp, state_forward1 = lstm_forward1(x[t], state_forward1)\n",
    "\tstates_forward1.append(state_forward1.h)\n",
    "\n",
    "\tp, state_backward1 = lstm_backward1(x[seq_len-1-t], state_backward1)\n",
    "\tstates_backward1.append(state_backward1.h)\n",
    "\n",
    "states_backward1.reverse()\n",
    "\n",
    "states_forward1 = tf.stack(states_forward1)\n",
    "states_backward1 = tf.stack(states_backward1)\n",
    "\n",
    "# state: \tseq_len x batch_size*(story_length+2) x num_units\n",
    "# sentences: batch_size*(story_length+2) x num_units\n",
    "# h_layer: seq_len x batch_size*(story_length+2) x num_units\n",
    "print(states_forward1.shape,states_backward1.shape)\n",
    "h_layer = tf.add(states_forward1,states_backward1)\n",
    "print(h_layer.shape)\n",
    "sentences = tf.reduce_mean(h_layer,axis=0)\n",
    "print(sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 200)\n",
      "(50, 4, 18)\n",
      "(18, 200)\n",
      "(200, 300)\n"
     ]
    }
   ],
   "source": [
    "##### Attention  Layer #####\n",
    "\n",
    "# attention: seq_len x batch_size*story_length x num_units\n",
    "h_layer = tf.reshape(h_layer,[seq_len,batch_size,story_length+2,num_units])\n",
    "\n",
    "attention = tf.layers.dense(tf.reshape(h_layer[:,:,:story_length,:],\\\n",
    "\t[seq_len*batch_size*story_length,num_units]), \n",
    "\tnum_units, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# alpha_correct: seq_len x batch_size*story_length\n",
    "attention = tf.reshape(attention, [seq_len, batch_size*story_length,num_units])\n",
    "alpha_correct = tf.reduce_sum(tf.multiply(attention,sentences[story_length]),axis=2)\n",
    "alpha_wrong = tf.reduce_sum(tf.multiply(attention,sentences[story_length+1]),axis=2)\n",
    "# alpha_wrong = tf.reduce_sum(tf.multiply(attention,sentences[story_length+1]),axis=1)\n",
    "print(alpha_correct.shape)\n",
    "\n",
    "alpha_correct = tf.transpose(tf.reshape(alpha_correct,[seq_len,batch_size,story_length]),perm=[1,2,0])\n",
    "alpha_wrong = tf.transpose(tf.reshape(alpha_wrong,[seq_len,batch_size,story_length]),perm=[1,2,0])\n",
    "\n",
    "# alpha_correct: batch_size x story_length x seq_len \n",
    "print(alpha_correct.shape)\n",
    "\n",
    "# beta_correct: batch_size x story_length x seq_len\n",
    "beta_correct = tf.nn.softmax(alpha_correct) \n",
    "beta_wrong = tf.nn.softmax(alpha_wrong) \n",
    "\n",
    "beta_correct = tf.transpose(tf.reshape(beta_correct,[batch_size*story_length,seq_len]))\n",
    "beta_wrong = tf.transpose(tf.reshape(beta_wrong,[batch_size*story_length,seq_len]))\n",
    "print(beta_correct.shape)\n",
    "\n",
    "# beta_correct: seq_len x batch_size*story_length\n",
    "# h_layer: seq_len x batch_size*(story_length+2) x num_units\n",
    "sentences_atten_correct = tf.multiply(tf.reshape(h_layer[:,:,:story_length,:],[-1,num_units]),tf.reshape(beta_correct,[-1,1]))\n",
    "sentences_atten_correct = tf.reduce_sum(tf.reshape(sentences_atten_correct,[seq_len,batch_size*story_length,-1]),axis=0)\n",
    "\n",
    "sentences_atten_wrong = tf.multiply(tf.reshape(h_layer[:,:,:story_length,:],[-1,num_units]),tf.reshape(beta_wrong,[-1,1]))\n",
    "sentences_atten_wrong = tf.reduce_sum(tf.reshape(sentences_atten_wrong,[seq_len,batch_size*story_length,-1]),axis=0)\n",
    "\n",
    "print(sentences_atten_correct.shape)\n",
    "# print(tf.multiply(tf.reshape(h_layer[:,:,:story_length,:],[-1,num_units]),tf.reshape(beta_correct,[-1,1])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 300)\n",
      "(100, 300)\n",
      "(50, 4, 300)\n",
      "(100, 4, 300)\n",
      "4\n",
      "(100, 300)\n"
     ]
    }
   ],
   "source": [
    "##### Embed Stories #####\n",
    "# correct and wrong sentences are putted together respectively\n",
    "\n",
    "sentences_atten_correct = tf.reshape(sentences_atten_correct,[batch_size,story_length,num_units])\n",
    "sentences_atten_wrong = tf.reshape(sentences_atten_wrong,[batch_size,story_length,num_units])\n",
    "\n",
    "endings = tf.reshape(sentences,[batch_size,story_length+2,num_units])[:,story_length:,:]\n",
    "print(endings[:,0,:].shape)\n",
    "endings = tf.concat([endings[:,0,:], endings[:,1,:]],axis=0)\n",
    "print(endings.shape)\n",
    "endings = tf.reshape(endings,[batch_size*2,-1])\n",
    "\n",
    "lstm_forward2 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "lstm_backward2 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "state_forward2 = lstm_forward2.zero_state(batch_size*2, 'float')\n",
    "state_backward2 = lstm_backward2.zero_state(batch_size*2, 'float')\n",
    "\n",
    "states_forward2 = []\n",
    "states_backward2 = []\n",
    "\n",
    "print(sentences_atten_correct.shape)\n",
    "\n",
    "sentence_att = tf.concat([sentences_atten_correct,sentences_atten_wrong],axis=0)\n",
    "\n",
    "\n",
    "print(sentence_att.shape)\n",
    "sentence_att_unstack = tf.unstack(sentence_att,story_length,1)\n",
    "\n",
    "print(len(sentence_att_unstack))\n",
    "\n",
    "for t in range(story_length):\n",
    "    \n",
    "\tp, state_forward2 = lstm_forward2(sentence_att_unstack[t], state_forward2)\n",
    "\tstates_forward2.append(state_forward2.h)\n",
    "\n",
    "\tp, state_backward2 = lstm_backward2(sentence_att_unstack[story_length-1-t], state_backward2)\n",
    "\tstates_backward2.append(state_backward2.h)\n",
    "\n",
    "# states_backward2.reverse()\n",
    "\n",
    "state_forward2 = lstm_forward2.zero_state(batch_size*2, 'float')\n",
    "state_backward2 = lstm_backward2.zero_state(batch_size*2, 'float')\n",
    "\n",
    "p, endings_forward = lstm_forward2(endings, state_forward2)\n",
    "p, endings_backward = lstm_backward2(endings, state_backward2)\n",
    "\n",
    "stories = tf.add(states_forward2[-1],states_backward2[-1])\n",
    "endings_embed = tf.add(endings_forward.h,endings_backward.h)\n",
    "\n",
    "print(stories.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 600)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "##### Calculate Score, loss, and do updating #####\n",
    "hier_stories = tf.concat([stories,endings_embed],axis=1)\n",
    "print(hier_stories.shape)\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale)\n",
    "hidden_layer = tf.layers.dense(hier_stories, 512, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                               activation=tf.nn.relu,kernel_regularizer=regularizer)\n",
    "\n",
    "scores = tf.layers.dense(hidden_layer, 1, kernel_regularizer=regularizer,#activation=tf.nn.tanh,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "# logits = tf.layers.dense(\n",
    "#     states, num_vocab, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "losses = tf.maximum(tf.add(tf.subtract(scores[batch_size:],scores[:batch_size]),1),0)\n",
    "print(losses.shape)\n",
    "loss = tf.reduce_sum(losses)\n",
    "\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "loss += l2_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()  # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 6, 18)\n",
      "epoch: 0 0 37 loss: 327.55389404296875 accuracy: 0.26\n",
      "epoch: 0 10 37 loss: 279.87127962979406 accuracy: 0.4854545454545454\n",
      "epoch: 0 20 37 loss: 244.5553232828776 accuracy: 0.5161904761904762\n",
      "epoch: 0 30 37 loss: 215.25428722750755 accuracy: 0.5335483870967742\n",
      "epoch_val: 0 loss: 108.54530334472656 accuracy: 0.6142857142857142\n",
      "epoch: 1 0 37 loss: 114.09099578857422 accuracy: 0.56\n",
      "epoch: 1 10 37 loss: 94.10007268732244 accuracy: 0.6545454545454545\n",
      "epoch: 1 20 37 loss: 83.2124613807315 accuracy: 0.6676190476190477\n",
      "epoch: 1 30 37 loss: 75.3611941183767 accuracy: 0.6793548387096774\n",
      "epoch_val: 1 loss: 54.59133802141462 accuracy: 0.64\n",
      "epoch: 2 0 37 loss: 39.004493713378906 accuracy: 0.8\n",
      "epoch: 2 10 37 loss: 41.033627249977805 accuracy: 0.770909090909091\n",
      "epoch: 2 20 37 loss: 39.6150392804827 accuracy: 0.7761904761904762\n",
      "epoch: 2 30 37 loss: 37.89852068501134 accuracy: 0.7767741935483873\n",
      "epoch_val: 2 loss: 43.92912728445871 accuracy: 0.6714285714285715\n",
      "epoch: 3 0 37 loss: 33.61701202392578 accuracy: 0.76\n",
      "epoch: 3 10 37 loss: 30.72858411615545 accuracy: 0.7818181818181817\n",
      "epoch: 3 20 37 loss: 28.23469915844145 accuracy: 0.8085714285714286\n",
      "epoch: 3 30 37 loss: 27.279229317941972 accuracy: 0.812258064516129\n",
      "epoch_val: 3 loss: 38.536798749651226 accuracy: 0.6685714285714287\n",
      "epoch: 4 0 37 loss: 26.891889572143555 accuracy: 0.86\n",
      "epoch: 4 10 37 loss: 24.306497140364215 accuracy: 0.8327272727272728\n",
      "epoch: 4 20 37 loss: 24.558864502679732 accuracy: 0.8238095238095239\n",
      "epoch: 4 30 37 loss: 23.62586058339765 accuracy: 0.8335483870967741\n",
      "epoch_val: 4 loss: 39.42317117963518 accuracy: 0.6742857142857144\n",
      "epoch: 5 0 37 loss: 28.0333309173584 accuracy: 0.78\n",
      "epoch: 5 10 37 loss: 19.150374499234285 accuracy: 0.8509090909090908\n",
      "epoch: 5 20 37 loss: 18.110748926798504 accuracy: 0.8657142857142859\n",
      "epoch: 5 30 37 loss: 15.686075525899087 accuracy: 0.8870967741935485\n",
      "epoch_val: 5 loss: 41.01578957693918 accuracy: 0.6514285714285715\n",
      "epoch: 6 0 37 loss: 23.11760139465332 accuracy: 0.88\n",
      "epoch: 6 10 37 loss: 22.13730708035556 accuracy: 0.8672727272727273\n",
      "epoch: 6 20 37 loss: 20.046772139413015 accuracy: 0.8742857142857142\n",
      "epoch: 6 30 37 loss: 18.671785416141635 accuracy: 0.8806451612903226\n",
      "epoch_val: 6 loss: 50.55042484828404 accuracy: 0.6628571428571429\n",
      "epoch: 7 0 37 loss: 19.655941009521484 accuracy: 0.86\n",
      "epoch: 7 10 37 loss: 14.73741375316273 accuracy: 0.8981818181818184\n",
      "epoch: 7 20 37 loss: 13.975861049833751 accuracy: 0.9019047619047621\n",
      "epoch: 7 30 37 loss: 12.59434566190166 accuracy: 0.9122580645161293\n",
      "epoch_val: 7 loss: 44.52965818132673 accuracy: 0.6657142857142857\n",
      "epoch: 8 0 37 loss: 7.347178936004639 accuracy: 0.96\n",
      "epoch: 8 10 37 loss: 9.729747685519131 accuracy: 0.9290909090909092\n",
      "epoch: 8 20 37 loss: 9.398735659463066 accuracy: 0.9342857142857147\n",
      "epoch: 8 30 37 loss: 8.654995910583004 accuracy: 0.9412903225806457\n",
      "epoch_val: 8 loss: 48.81118311200823 accuracy: 0.6485714285714286\n",
      "epoch: 9 0 37 loss: 11.11682415008545 accuracy: 0.94\n",
      "epoch: 9 10 37 loss: 9.979770096865566 accuracy: 0.9399999999999998\n",
      "epoch: 9 20 37 loss: 7.811732133229573 accuracy: 0.9533333333333334\n",
      "epoch: 9 30 37 loss: 7.2560871647250265 accuracy: 0.9600000000000002\n",
      "epoch_val: 9 loss: 56.90702765328543 accuracy: 0.6571428571428571\n",
      "epoch: 0 0 37 loss: 328.21624755859375 accuracy: 0.36\n",
      "epoch: 0 10 37 loss: 277.7787919477983 accuracy: 0.6181818181818183\n",
      "epoch: 0 20 37 loss: 238.00325520833334 accuracy: 0.6019047619047619\n",
      "epoch: 0 30 37 loss: 206.71148484753024 accuracy: 0.5916129032258064\n",
      "epoch_val: 0 loss: 94.16014426095145 accuracy: 0.6485714285714287\n",
      "epoch: 1 0 37 loss: 86.61865234375 accuracy: 0.66\n",
      "epoch: 1 10 37 loss: 79.2630386352539 accuracy: 0.6690909090909091\n",
      "epoch: 1 20 37 loss: 70.31934175037202 accuracy: 0.6676190476190477\n",
      "epoch: 1 30 37 loss: 61.264186243857104 accuracy: 0.69741935483871\n",
      "epoch_val: 1 loss: 45.492942810058594 accuracy: 0.6885714285714286\n",
      "epoch: 2 0 37 loss: 28.356369018554688 accuracy: 0.82\n",
      "epoch: 2 10 37 loss: 32.15277654474432 accuracy: 0.7654545454545455\n",
      "epoch: 2 20 37 loss: 31.10884766351609 accuracy: 0.7704761904761904\n",
      "epoch: 2 30 37 loss: 30.865601816485004 accuracy: 0.7735483870967744\n",
      "epoch_val: 2 loss: 38.303127833775115 accuracy: 0.7000000000000001\n",
      "epoch: 3 0 37 loss: 26.309619903564453 accuracy: 0.74\n",
      "epoch: 3 10 37 loss: 26.933677846735176 accuracy: 0.7909090909090909\n",
      "epoch: 3 20 37 loss: 24.546286673772904 accuracy: 0.8123809523809525\n",
      "epoch: 3 30 37 loss: 24.763870516131 accuracy: 0.8038709677419356\n",
      "epoch_val: 3 loss: 37.623784746442524 accuracy: 0.6857142857142857\n",
      "epoch: 4 0 37 loss: 29.56270408630371 accuracy: 0.76\n",
      "epoch: 4 10 37 loss: 23.188587362116035 accuracy: 0.8072727272727271\n",
      "epoch: 4 20 37 loss: 21.30821059999012 accuracy: 0.8314285714285713\n",
      "epoch: 4 30 37 loss: 20.687737095740534 accuracy: 0.8432258064516128\n",
      "epoch_val: 4 loss: 38.29263087681362 accuracy: 0.7057142857142856\n",
      "epoch: 5 0 37 loss: 29.246122360229492 accuracy: 0.82\n",
      "epoch: 5 10 37 loss: 17.70853918248957 accuracy: 0.8654545454545454\n",
      "epoch: 5 20 37 loss: 18.216228893824987 accuracy: 0.8647619047619047\n",
      "epoch: 5 30 37 loss: 18.191926863885694 accuracy: 0.861290322580645\n",
      "epoch_val: 5 loss: 45.319448471069336 accuracy: 0.6942857142857143\n",
      "epoch: 6 0 37 loss: 12.246222496032715 accuracy: 0.94\n",
      "epoch: 6 10 37 loss: 13.890228921716863 accuracy: 0.9127272727272728\n",
      "epoch: 6 20 37 loss: 13.00445588429769 accuracy: 0.9171428571428574\n",
      "epoch: 6 30 37 loss: 12.202534921707645 accuracy: 0.9206451612903229\n",
      "epoch_val: 6 loss: 49.32116862705776 accuracy: 0.6828571428571429\n",
      "epoch: 7 0 37 loss: 3.8029913902282715 accuracy: 1.0\n",
      "epoch: 7 10 37 loss: 8.012238025665283 accuracy: 0.9381818181818183\n",
      "epoch: 7 20 37 loss: 9.518702416192918 accuracy: 0.9342857142857145\n",
      "epoch: 7 30 37 loss: 8.835385930153631 accuracy: 0.941935483870968\n",
      "epoch_val: 7 loss: 61.876878465924946 accuracy: 0.6828571428571427\n",
      "epoch: 8 0 37 loss: 5.422636985778809 accuracy: 0.96\n",
      "epoch: 8 10 37 loss: 9.2961782542142 accuracy: 0.9436363636363637\n",
      "epoch: 8 20 37 loss: 8.34789328348069 accuracy: 0.946666666666667\n",
      "epoch: 8 30 37 loss: 7.299181740130147 accuracy: 0.9548387096774198\n",
      "epoch_val: 8 loss: 64.0142800467355 accuracy: 0.6799999999999999\n",
      "epoch: 9 0 37 loss: 2.131138324737549 accuracy: 1.0\n",
      "epoch: 9 10 37 loss: 6.414627595381304 accuracy: 0.9654545454545456\n",
      "epoch: 9 20 37 loss: 5.224291557357425 accuracy: 0.9723809523809527\n",
      "epoch: 9 30 37 loss: 4.820282509250026 accuracy: 0.9748387096774197\n",
      "epoch_val: 9 loss: 57.6874144417899 accuracy: 0.6685714285714285\n",
      "epoch: 0 0 37 loss: 327.57025146484375 accuracy: 0.52\n",
      "epoch: 0 10 37 loss: 283.871469671076 accuracy: 0.5345454545454547\n",
      "epoch: 0 20 37 loss: 246.63048589797248 accuracy: 0.5533333333333333\n",
      "epoch: 0 30 37 loss: 217.34654334283644 accuracy: 0.5625806451612902\n",
      "epoch_val: 0 loss: 114.41160147530692 accuracy: 0.58\n",
      "epoch: 1 0 37 loss: 114.79678344726562 accuracy: 0.62\n",
      "epoch: 1 10 37 loss: 102.53211628306995 accuracy: 0.5545454545454547\n",
      "epoch: 1 20 37 loss: 91.2334474836077 accuracy: 0.5914285714285715\n",
      "epoch: 1 30 37 loss: 81.92023677210653 accuracy: 0.6148387096774193\n",
      "epoch_val: 1 loss: 52.274605887276785 accuracy: 0.7114285714285715\n",
      "epoch: 2 0 37 loss: 46.228172302246094 accuracy: 0.72\n",
      "epoch: 2 10 37 loss: 44.734192934903234 accuracy: 0.7218181818181818\n",
      "epoch: 2 20 37 loss: 41.57843144734701 accuracy: 0.74\n",
      "epoch: 2 30 37 loss: 39.69192381828062 accuracy: 0.7406451612903227\n",
      "epoch_val: 2 loss: 39.43150193350656 accuracy: 0.72\n",
      "epoch: 3 0 37 loss: 32.77946472167969 accuracy: 0.72\n",
      "epoch: 3 10 37 loss: 33.843904148448594 accuracy: 0.7527272727272727\n",
      "epoch: 3 20 37 loss: 31.07251212710426 accuracy: 0.7761904761904761\n",
      "epoch: 3 30 37 loss: 29.63848481639739 accuracy: 0.7890322580645163\n",
      "epoch_val: 3 loss: 41.52488654000418 accuracy: 0.7057142857142856\n",
      "epoch: 4 0 37 loss: 37.16703414916992 accuracy: 0.72\n",
      "epoch: 4 10 37 loss: 33.273869601163 accuracy: 0.7490909090909091\n",
      "epoch: 4 20 37 loss: 31.776859555925643 accuracy: 0.7619047619047619\n",
      "epoch: 4 30 37 loss: 30.011422618742913 accuracy: 0.7767741935483872\n",
      "epoch_val: 4 loss: 41.56353541782924 accuracy: 0.6542857142857142\n",
      "epoch: 5 0 37 loss: 23.847726821899414 accuracy: 0.82\n",
      "epoch: 5 10 37 loss: 30.92465001886541 accuracy: 0.7618181818181818\n",
      "epoch: 5 20 37 loss: 30.523550124395463 accuracy: 0.7542857142857142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 30 37 loss: 28.78261375427246 accuracy: 0.7716129032258064\n",
      "epoch_val: 5 loss: 39.333244051252095 accuracy: 0.6857142857142857\n",
      "epoch: 6 0 37 loss: 21.1915340423584 accuracy: 0.86\n",
      "epoch: 6 10 37 loss: 21.954714688387785 accuracy: 0.8254545454545454\n",
      "epoch: 6 20 37 loss: 21.551562808808825 accuracy: 0.8323809523809526\n",
      "epoch: 6 30 37 loss: 20.822664137809507 accuracy: 0.8412903225806451\n",
      "epoch_val: 6 loss: 40.535323006766184 accuracy: 0.6828571428571429\n",
      "epoch: 7 0 37 loss: 25.809383392333984 accuracy: 0.8\n",
      "epoch: 7 10 37 loss: 21.13849232413552 accuracy: 0.8418181818181819\n",
      "epoch: 7 20 37 loss: 21.555408704848517 accuracy: 0.8438095238095239\n",
      "epoch: 7 30 37 loss: 20.05642057234241 accuracy: 0.8535483870967744\n",
      "epoch_val: 7 loss: 38.299046652657644 accuracy: 0.72\n",
      "epoch: 8 0 37 loss: 24.82274627685547 accuracy: 0.78\n",
      "epoch: 8 10 37 loss: 28.718185251409356 accuracy: 0.8090909090909091\n",
      "epoch: 8 20 37 loss: 33.382087071736656 accuracy: 0.7638095238095237\n",
      "epoch: 8 30 37 loss: 31.85528619827763 accuracy: 0.7806451612903227\n",
      "epoch_val: 8 loss: 41.48708398001535 accuracy: 0.6857142857142858\n",
      "epoch: 9 0 37 loss: 27.234405517578125 accuracy: 0.76\n",
      "epoch: 9 10 37 loss: 22.155515497381035 accuracy: 0.84\n",
      "epoch: 9 20 37 loss: 21.935914357503254 accuracy: 0.8400000000000001\n",
      "epoch: 9 30 37 loss: 20.804394014420048 accuracy: 0.843225806451613\n",
      "epoch_val: 9 loss: 42.00070871625628 accuracy: 0.6685714285714285\n",
      "epoch: 0 0 37 loss: 326.98565673828125 accuracy: 0.22\n",
      "epoch: 0 10 37 loss: 284.9289661754261 accuracy: 0.46727272727272734\n",
      "epoch: 0 20 37 loss: 245.78352719261534 accuracy: 0.5295238095238095\n",
      "epoch: 0 30 37 loss: 216.2802040346207 accuracy: 0.5477419354838708\n",
      "epoch_val: 0 loss: 112.3666763305664 accuracy: 0.6171428571428572\n",
      "epoch: 1 0 37 loss: 114.78291320800781 accuracy: 0.54\n",
      "epoch: 1 10 37 loss: 100.61315432461825 accuracy: 0.5963636363636364\n",
      "epoch: 1 20 37 loss: 88.37530154273624 accuracy: 0.6342857142857142\n",
      "epoch: 1 30 37 loss: 80.28125836772304 accuracy: 0.6554838709677419\n",
      "epoch_val: 1 loss: 52.60990360804966 accuracy: 0.6914285714285715\n",
      "epoch: 2 0 37 loss: 59.201622009277344 accuracy: 0.72\n",
      "epoch: 2 10 37 loss: 47.866078810258344 accuracy: 0.7181818181818181\n",
      "epoch: 2 20 37 loss: 45.87611788795108 accuracy: 0.7238095238095238\n",
      "epoch: 2 30 37 loss: 44.60013678766066 accuracy: 0.730967741935484\n",
      "epoch_val: 2 loss: 42.29308646065848 accuracy: 0.697142857142857\n",
      "epoch: 3 0 37 loss: 33.44281005859375 accuracy: 0.74\n",
      "epoch: 3 10 37 loss: 37.95680340853605 accuracy: 0.7381818181818183\n",
      "epoch: 3 20 37 loss: 36.69049035935175 accuracy: 0.742857142857143\n",
      "epoch: 3 30 37 loss: 34.085136967320594 accuracy: 0.7580645161290324\n",
      "epoch_val: 3 loss: 37.37733513968332 accuracy: 0.72\n",
      "epoch: 4 0 37 loss: 30.031539916992188 accuracy: 0.8\n",
      "epoch: 4 10 37 loss: 19.122011358087715 accuracy: 0.8781818181818182\n",
      "epoch: 4 20 37 loss: 22.98745500473749 accuracy: 0.8457142857142858\n",
      "epoch: 4 30 37 loss: 27.819906850014963 accuracy: 0.8038709677419356\n",
      "epoch_val: 4 loss: 48.26446206229074 accuracy: 0.6628571428571428\n",
      "epoch: 5 0 37 loss: 26.639209747314453 accuracy: 0.84\n",
      "epoch: 5 10 37 loss: 29.903596704656426 accuracy: 0.7836363636363637\n",
      "epoch: 5 20 37 loss: 27.59713636125837 accuracy: 0.802857142857143\n",
      "epoch: 5 30 37 loss: 26.531897944788778 accuracy: 0.8103225806451614\n",
      "epoch_val: 5 loss: 36.90530913216727 accuracy: 0.6799999999999999\n",
      "epoch: 6 0 37 loss: 20.821800231933594 accuracy: 0.86\n",
      "epoch: 6 10 37 loss: 24.04806033047763 accuracy: 0.8181818181818182\n",
      "epoch: 6 20 37 loss: 24.496620178222656 accuracy: 0.8133333333333332\n",
      "epoch: 6 30 37 loss: 22.60175455770185 accuracy: 0.8354838709677419\n",
      "epoch_val: 6 loss: 42.52883202689035 accuracy: 0.702857142857143\n",
      "epoch: 7 0 37 loss: 19.594797134399414 accuracy: 0.8\n",
      "epoch: 7 10 37 loss: 19.315578720786355 accuracy: 0.8581818181818182\n",
      "epoch: 7 20 37 loss: 18.21494152432396 accuracy: 0.8666666666666666\n",
      "epoch: 7 30 37 loss: 17.22898993953582 accuracy: 0.8748387096774193\n",
      "epoch_val: 7 loss: 43.71226842062814 accuracy: 0.7057142857142856\n",
      "epoch: 8 0 37 loss: 1.495581030845642 accuracy: 1.0\n",
      "epoch: 8 10 37 loss: 42.53108087452975 accuracy: 0.7181818181818183\n",
      "epoch: 8 20 37 loss: 38.17034327416193 accuracy: 0.74\n",
      "epoch: 8 30 37 loss: 35.27089984186234 accuracy: 0.756774193548387\n",
      "epoch_val: 8 loss: 38.57496861049107 accuracy: 0.7257142857142858\n",
      "epoch: 9 0 37 loss: 17.50483512878418 accuracy: 0.9\n",
      "epoch: 9 10 37 loss: 23.105572613802824 accuracy: 0.8472727272727275\n",
      "epoch: 9 20 37 loss: 21.794707252865745 accuracy: 0.8552380952380952\n",
      "epoch: 9 30 37 loss: 19.214127571352066 accuracy: 0.8767741935483874\n",
      "epoch_val: 9 loss: 39.087671416146414 accuracy: 0.722857142857143\n",
      "epoch: 0 0 37 loss: 326.7633361816406 accuracy: 0.56\n",
      "epoch: 0 10 37 loss: 276.8896206942472 accuracy: 0.6872727272727273\n",
      "epoch: 0 20 37 loss: 240.58560907273065 accuracy: 0.6266666666666667\n",
      "epoch: 0 30 37 loss: 209.4662876744424 accuracy: 0.6283870967741936\n",
      "epoch_val: 0 loss: 97.65051160539899 accuracy: 0.62\n",
      "epoch: 1 0 37 loss: 105.76053619384766 accuracy: 0.62\n",
      "epoch: 1 10 37 loss: 76.52142888849431 accuracy: 0.7345454545454545\n",
      "epoch: 1 20 37 loss: 79.32654753185454 accuracy: 0.7085714285714285\n",
      "epoch: 1 30 37 loss: 73.05236065772272 accuracy: 0.6980645161290322\n",
      "epoch_val: 1 loss: 53.61221531459263 accuracy: 0.66\n",
      "epoch: 2 0 37 loss: 49.76293182373047 accuracy: 0.64\n",
      "epoch: 2 10 37 loss: 49.96247933127663 accuracy: 0.6927272727272726\n",
      "epoch: 2 20 37 loss: 45.99380220685686 accuracy: 0.7171428571428571\n",
      "epoch: 2 30 37 loss: 44.52509369388704 accuracy: 0.7174193548387096\n",
      "epoch_val: 2 loss: 42.67107391357422 accuracy: 0.6799999999999999\n",
      "epoch: 3 0 37 loss: 42.18266296386719 accuracy: 0.66\n",
      "epoch: 3 10 37 loss: 35.99710533835671 accuracy: 0.7309090909090908\n",
      "epoch: 3 20 37 loss: 36.02581569126674 accuracy: 0.736190476190476\n",
      "epoch: 3 30 37 loss: 34.122624366514145 accuracy: 0.7458064516129032\n",
      "epoch_val: 3 loss: 40.52519498552595 accuracy: 0.6742857142857143\n",
      "epoch: 4 0 37 loss: 27.189125061035156 accuracy: 0.82\n",
      "epoch: 4 10 37 loss: 29.503971966830168 accuracy: 0.7854545454545453\n",
      "epoch: 4 20 37 loss: 29.391523815336683 accuracy: 0.7914285714285715\n",
      "epoch: 4 30 37 loss: 28.928023430608935 accuracy: 0.7916129032258065\n",
      "epoch_val: 4 loss: 34.050052642822266 accuracy: 0.7142857142857143\n",
      "epoch: 5 0 37 loss: 21.118755340576172 accuracy: 0.9\n",
      "epoch: 5 10 37 loss: 28.712356220592152 accuracy: 0.8\n",
      "epoch: 5 20 37 loss: 27.767777760823567 accuracy: 0.8066666666666669\n",
      "epoch: 5 30 37 loss: 26.528626072791315 accuracy: 0.8122580645161294\n",
      "epoch_val: 5 loss: 30.02804606301444 accuracy: 0.76\n",
      "epoch: 6 0 37 loss: 8.604440689086914 accuracy: 0.94\n",
      "epoch: 6 10 37 loss: 18.054806102405895 accuracy: 0.889090909090909\n",
      "epoch: 6 20 37 loss: 26.122075228464034 accuracy: 0.816190476190476\n",
      "epoch: 6 30 37 loss: 28.826398118849724 accuracy: 0.7948387096774193\n",
      "epoch_val: 6 loss: 40.437223706926616 accuracy: 0.6571428571428571\n",
      "epoch: 7 0 37 loss: 26.785961151123047 accuracy: 0.9\n",
      "epoch: 7 10 37 loss: 28.585012609308418 accuracy: 0.7999999999999999\n",
      "epoch: 7 20 37 loss: 27.61731111435663 accuracy: 0.7990476190476189\n",
      "epoch: 7 30 37 loss: 26.45541166490124 accuracy: 0.8103225806451612\n",
      "epoch_val: 7 loss: 37.94459615434919 accuracy: 0.7000000000000001\n",
      "epoch: 8 0 37 loss: 12.076482772827148 accuracy: 0.92\n",
      "epoch: 8 10 37 loss: 22.197653336958453 accuracy: 0.8309090909090909\n",
      "epoch: 8 20 37 loss: 21.421366328284854 accuracy: 0.8342857142857144\n",
      "epoch: 8 30 37 loss: 20.740363797833844 accuracy: 0.8400000000000002\n",
      "epoch_val: 8 loss: 38.31587355477469 accuracy: 0.6971428571428572\n",
      "epoch: 9 0 37 loss: 17.21600341796875 accuracy: 0.88\n",
      "epoch: 9 10 37 loss: 16.94088684428822 accuracy: 0.8927272727272727\n",
      "epoch: 9 20 37 loss: 18.772840636117117 accuracy: 0.8695238095238096\n",
      "epoch: 9 30 37 loss: 17.774993496556437 accuracy: 0.8761290322580645\n",
      "epoch_val: 9 loss: 27.683678218296595 accuracy: 0.7828571428571429\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# Train the model\n",
    "# ----------------------------------------------\n",
    "epoch = 10\n",
    "fold_num = 5\n",
    "\n",
    "# X_Train.shape: (88161, 6, 19)\n",
    "print(X_ori.shape)\n",
    "\n",
    "single_fold = X_train.shape[0]//10\n",
    "\n",
    "###### remove the weird 2nd part #####\n",
    "# X_train = np.concatenate((X_train[:1*single_fold],X_train[2*single_fold:])) \n",
    "# single_fold = X_ori.shape[0]//10\n",
    "# X_train = X_ori[:9*single_fold] \n",
    "# X_test = X_ori[9*single_fold:] \n",
    "# print(X_test.shape)\n",
    "\n",
    "X_train = X_ori\n",
    "############################################\n",
    "\n",
    "np.random.shuffle(X_train)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    single_fold = X_train.shape[0]//fold_num\n",
    "    \n",
    "    for k in range(fold_num):         \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        X_train_temp = np.concatenate((X_train[:k*single_fold],X_train[(k+1)*single_fold:]))   \n",
    "        \n",
    "        X_val = X_train[k*single_fold:(k+1)*single_fold]\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            accuracy = 0\n",
    "            loss_mean = 0\n",
    "            for j in range(X_train.shape[0]//batch_size):\n",
    "                         \n",
    "                X_batch = X_train_temp[np.random.choice(X_train_temp.shape[0],batch_size)]\n",
    "\n",
    "                X_batch_embed = np.zeros((batch_size,(story_length+2), seq_len, embedding_dim))\n",
    "                X_batch_embed = embedding_matrix[X_batch]\n",
    "                X_batch_embed = np.reshape(X_batch_embed,(batch_size*(story_length+2), seq_len, embedding_dim))\n",
    "\n",
    "                score, loss_sofar, _ = sess.run([scores,loss,train_step],feed_dict = {embedding: X_batch_embed})\n",
    "                accuracy += np.mean((score[:batch_size]-score[batch_size:])>0)\n",
    "                loss_mean += loss_sofar\n",
    "    #             print(score.reshape([-1])[:batch_size])\n",
    "    #             print(score.reshape([-1])[batch_size:])\n",
    "\n",
    "                if(j % 10 == 0):\n",
    "                    print(\"epoch:\",i,j, X_train.shape[0]//batch_size,\"loss:\",loss_mean/(j+1),\"accuracy:\",accuracy/(j+1))\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, './saved/model_biLSTM_att_val_test_'+str(k)+'.ckpt')\n",
    "        \n",
    "            # validation\n",
    "            accuracy = 0\n",
    "            loss_mean = 0\n",
    "            for j in range(single_fold//batch_size):\n",
    "                X_val_temp = X_val[j*batch_size:(j+1)*batch_size]\n",
    "                X_val_embed = embedding_matrix[X_val_temp]\n",
    "                \n",
    "                X_val_embed = np.reshape(X_val_embed,(batch_size*(story_length+2), seq_len, embedding_dim))\n",
    "                \n",
    "            \n",
    "                score, loss_sofar = sess.run([scores,loss],feed_dict = {embedding: X_val_embed})\n",
    "                accuracy += np.mean((score[:batch_size]-score[batch_size:])>0)\n",
    "                loss_mean += loss_sofar\n",
    "            \n",
    "            accuracy /= (single_fold//batch_size)\n",
    "            loss_mean /= (single_fold//batch_size)\n",
    "            print(\"epoch_val:\",i, \"loss:\",loss_mean,\"accuracy:\",accuracy)\n",
    "           \n",
    "        \n",
    "#             accuracy = 0\n",
    "#             loss_mean = 0\n",
    "#             for j in range(X_test.shape[0]//batch_size):\n",
    "#                 X_test_temp = X_test[j*batch_size:(j+1)*batch_size]\n",
    "#                 X_test_embed = embedding_matrix[X_test_temp]\n",
    "\n",
    "#                 X_test_embed = np.reshape(X_test_embed,(batch_size*(story_length+2), seq_len, embedding_dim))\n",
    "\n",
    "\n",
    "#                 score, loss_sofar = sess.run([scores,loss],feed_dict = {embedding: X_test_embed})\n",
    "#                 accuracy += np.mean((score[:batch_size]-score[batch_size:])>0)\n",
    "#                 loss_mean += loss_sofar\n",
    "\n",
    "#             accuracy /= (X_test.shape[0]//batch_size)\n",
    "#             loss_mean /= (X_test.shape[0]//batch_size)\n",
    "#             print(\"epoch_test:\", \"loss:\",loss_mean,\"accuracy:\",accuracy)\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
