{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text as kpt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Constants\n",
    "# ----------------------------------------------\n",
    "num_units = 300\n",
    "batch_size = 50\n",
    "embedding_dim = 300\n",
    "story_length = 4\n",
    "scale = 1.0\n",
    "seq_len = 30\n",
    "# num_vocab = 20 # 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 10)\n",
      "7432\n",
      "11226\n",
      "(1871, 6, 30)\n",
      "(0, 30)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# Preprocess the sentences (Modified from Hong's code)\n",
    "# ----------------------------------------------\n",
    "##### Load data #####\n",
    "# TRAIN_URL = 'train_stories.csv'\n",
    "TRAIN_URL = 'eval_stories.csv'\n",
    "\n",
    "# InputStoryid,InputSentence1,InputSentence2,InputSentence3,InputSentence4,RandomFifthSentenceQuiz1,RandomFifthSentenceQuiz2,AnswerRightEnding\n",
    "df_train_sep = pd.read_csv(TRAIN_URL).iloc[:, 1:]\n",
    "\n",
    "df_train_sep['correct'] = df_train_sep[['InputSentence1','InputSentence2','InputSentence3',\\\n",
    "                                        'InputSentence4','RandomFifthSentenceQuiz1','RandomFifthSentenceQuiz2']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "df_train_sep['right_endings'] = df_train_sep['RandomFifthSentenceQuiz1']\n",
    "df_train_sep['right_endings'].update(df_train_sep.loc[df_train_sep['AnswerRightEnding'] == 2,'RandomFifthSentenceQuiz2'])\n",
    "\n",
    "df_train_sep['wrong_endings'] = df_train_sep['RandomFifthSentenceQuiz2']\n",
    "df_train_sep['wrong_endings'].update(df_train_sep.loc[df_train_sep['AnswerRightEnding'] == 2,'RandomFifthSentenceQuiz1'])\n",
    "\n",
    "print(df_train_sep.shape)\n",
    "\n",
    "\n",
    "##### Prepare dictionary #####\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenizer = kpt.Tokenizer(oov_token='<UNK>', num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train_sep['correct'])\n",
    "tokenizer.word_index = {w:i for w,i in tokenizer.word_index.items() if i < VOCAB_SIZE}\n",
    "\n",
    "vocab_dict = tokenizer.word_index\n",
    "print(len(vocab_dict)+1)\n",
    "\n",
    "##### Prepare for training set #####\n",
    "# X_train = pd.concat([df_train_sep['sentence1'], df_train_sep['sentence2'],\\\n",
    "# \tdf_train_sep['sentence3'], df_train_sep['sentence4'], df_train_sep['sentence5']])\n",
    "\n",
    "X_train = pd.concat([df_train_sep['InputSentence1'], df_train_sep['InputSentence2'],\\\n",
    "\tdf_train_sep['InputSentence3'], df_train_sep['InputSentence4'], \\\n",
    "                     df_train_sep['RandomFifthSentenceQuiz1'], df_train_sep['RandomFifthSentenceQuiz2']])\n",
    "\n",
    "print(len(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# seq_len = max(len(s) for s in X_train)\n",
    "n_correct = df_train_sep.shape[0]\n",
    "# n_samples = n_correct * 2\n",
    "\n",
    "X_train = np.zeros((n_correct,6,seq_len),dtype=np.int32)\n",
    "\n",
    "for i in range(1,5):\n",
    "\tX_train_temp = tokenizer.texts_to_sequences(df_train_sep[\"InputSentence\"+str(i)])\n",
    "\n",
    "\tX_train_temp = pad_sequences(X_train_temp, maxlen=seq_len, padding='post')\n",
    "\tX_train[:,i-1,:] = X_train_temp\n",
    "\n",
    "\n",
    "X_train_temp = tokenizer.texts_to_sequences(df_train_sep[\"right_endings\"])\n",
    "X_train_temp = pad_sequences(X_train_temp, maxlen=seq_len, padding='post')\n",
    "X_train[:,4,:] = X_train_temp\n",
    "\n",
    "X_train_temp = tokenizer.texts_to_sequences(df_train_sep[\"wrong_endings\"])\n",
    "X_train_temp = pad_sequences(X_train_temp, maxlen=seq_len, padding='post')\n",
    "X_train[:,5,:] = X_train_temp\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[n_correct:,4,:].shape)\n",
    "\n",
    "X_ori = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 10)\n",
      "11226\n"
     ]
    }
   ],
   "source": [
    "##### Load test data #####\n",
    "TEST_URL = \"cloze_test_test__spring2016 - cloze_test_ALL_test.csv\"\n",
    "\n",
    "# InputStoryid,InputSentence1,InputSentence2,InputSentence3,InputSentence4,RandomFifthSentenceQuiz1,RandomFifthSentenceQuiz2,AnswerRightEnding\n",
    "df_test_sep = pd.read_csv(TRAIN_URL).iloc[:, 1:]\n",
    "\n",
    "df_test_sep['correct'] = df_test_sep[['InputSentence1','InputSentence2','InputSentence3',\\\n",
    "                                        'InputSentence4','RandomFifthSentenceQuiz1','RandomFifthSentenceQuiz2']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "df_test_sep['right_endings'] = df_test_sep['RandomFifthSentenceQuiz1']\n",
    "df_test_sep['right_endings'].update(df_test_sep.loc[df_test_sep['AnswerRightEnding'] == 2,'RandomFifthSentenceQuiz2'])\n",
    "\n",
    "df_test_sep['wrong_endings'] = df_test_sep['RandomFifthSentenceQuiz2']\n",
    "df_test_sep['wrong_endings'].update(df_test_sep.loc[df_test_sep['AnswerRightEnding'] == 2,'RandomFifthSentenceQuiz1'])\n",
    "\n",
    "print(df_test_sep.shape)\n",
    "\n",
    "##### Prepare for testing set #####\n",
    "# X_test = pd.concat([df_test_sep['sentence1'], df_test_sep['sentence2'],\\\n",
    "# \tdf_test_sep['sentence3'], df_test_sep['sentence4'], df_test_sep['sentence5']])\n",
    "\n",
    "X_test = pd.concat([df_test_sep['InputSentence1'], df_test_sep['InputSentence2'],\\\n",
    "\tdf_test_sep['InputSentence3'], df_test_sep['InputSentence4'], \\\n",
    "                     df_test_sep['RandomFifthSentenceQuiz1'], df_test_sep['RandomFifthSentenceQuiz2']])\n",
    "\n",
    "print(len(X_test))\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# seq_len = max(max(len(s) for s in X_test),seq_len)\n",
    "n_correct = df_test_sep.shape[0]\n",
    "# n_samples = n_correct * 2\n",
    "\n",
    "X_test = np.zeros((n_correct,6,seq_len),dtype=np.int32)\n",
    "\n",
    "for i in range(1,5):\n",
    "\tX_test_temp = tokenizer.texts_to_sequences(df_test_sep[\"InputSentence\"+str(i)])\n",
    "\n",
    "\tX_test_temp = pad_sequences(X_test_temp, maxlen=seq_len, padding='post')\n",
    "\tX_test[:,i-1,:] = X_test_temp\n",
    "\n",
    "\n",
    "X_test_temp = tokenizer.texts_to_sequences(df_test_sep[\"right_endings\"])\n",
    "X_test_temp = pad_sequences(X_test_temp, maxlen=seq_len, padding='post')\n",
    "X_test[:,4,:] = X_test_temp\n",
    "\n",
    "X_test_temp = tokenizer.texts_to_sequences(df_test_sep[\"wrong_endings\"])\n",
    "X_test_temp = pad_sequences(X_test_temp, maxlen=seq_len, padding='post')\n",
    "X_test[:,5,:] = X_test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(20000, 300) 7105\n"
     ]
    }
   ],
   "source": [
    "##### load embeddings #####\n",
    "embedding_dim = 300\n",
    "GLOVE_URL = 'glove.6B.300d.txt'\n",
    "\n",
    "embeddings = {}\n",
    "with open(GLOVE_URL, 'r', encoding='utf-8') as f:\n",
    "\tfor line in f:\n",
    "\t\tvalues = line.split()\n",
    "\t\tw = values[0]\n",
    "\t\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\t\tembeddings[w] = coefs\n",
    "\n",
    "\n",
    "print(len(embeddings))\n",
    "\n",
    "embedding_matrix = np.random.uniform(-1, 1, size=(VOCAB_SIZE, embedding_dim))\n",
    "num_loaded = 0\n",
    "for w, i in vocab_dict.items():\n",
    "\tif w in embeddings and i < VOCAB_SIZE:\n",
    "\t\tembedding_matrix[i] = embeddings[w]\n",
    "\t\tnum_loaded += 1\n",
    "\n",
    "print(embedding_matrix.shape, num_loaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 300, 300) (30, 300, 300)\n",
      "(30, 300, 300)\n",
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# Set up the graph\n",
    "# ----------------------------------------------\n",
    "tf.reset_default_graph()\n",
    "\n",
    "##### embed sentences #####\n",
    "lstm_forward1 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "\t\t\t\t\t\t\t   initializer=tf.contrib.layers.xavier_initializer())\n",
    "lstm_backward1 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "\t\t\t\t\t\t\t   initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# should use GloVe to embed words first (not finished!)\n",
    "# same story are close to each other\n",
    "\n",
    "embedding = tf.placeholder('float32', [batch_size*(story_length+2), seq_len, embedding_dim])\n",
    "\n",
    "x = tf.unstack(embedding, seq_len, 1)\n",
    "\n",
    "state_forward1 = lstm_forward1.zero_state(batch_size*(story_length+2), 'float')\n",
    "state_backward1 = lstm_backward1.zero_state(batch_size*(story_length+2), 'float')\n",
    "\n",
    "states_forward1 = []\n",
    "states_backward1 = []\n",
    "for t in range(seq_len):\n",
    "\n",
    "\tp, state_forward1 = lstm_forward1(x[t], state_forward1)\n",
    "\tstates_forward1.append(state_forward1.h)\n",
    "\n",
    "\tp, state_backward1 = lstm_backward1(x[seq_len-1-t], state_backward1)\n",
    "\tstates_backward1.append(state_backward1.h)\n",
    "\n",
    "states_backward1.reverse()\n",
    "\n",
    "states_forward1 = tf.stack(states_forward1)\n",
    "states_backward1 = tf.stack(states_backward1)\n",
    "\n",
    "# state: \tseq_len x batch_size*(story_length+2) x num_units\n",
    "# sentences: batch_size*(story_length+2) x num_units\n",
    "# h_layer: seq_len x batch_size*(story_length+2) x num_units\n",
    "print(states_forward1.shape,states_backward1.shape)\n",
    "h_layer = tf.add(states_forward1,states_backward1)\n",
    "print(h_layer.shape)\n",
    "sentences = tf.reduce_mean(h_layer,axis=0)\n",
    "print(sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 200)\n",
      "(50, 4, 30)\n",
      "(30, 200)\n",
      "(200, 300)\n"
     ]
    }
   ],
   "source": [
    "##### Attention  Layer #####\n",
    "\n",
    "# attention: seq_len x batch_size*story_length x num_units\n",
    "h_layer = tf.reshape(h_layer,[seq_len,batch_size,story_length+2,num_units])\n",
    "\n",
    "attention = tf.layers.dense(tf.reshape(h_layer[:,:,:story_length,:],\\\n",
    "\t[seq_len*batch_size*story_length,num_units]), \n",
    "\tnum_units, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# alpha_correct: seq_len x batch_size*story_length\n",
    "attention = tf.reshape(attention, [seq_len, batch_size*story_length,num_units])\n",
    "alpha_correct = tf.reduce_sum(tf.multiply(attention,sentences[story_length]),axis=2)\n",
    "alpha_wrong = tf.reduce_sum(tf.multiply(attention,sentences[story_length+1]),axis=2)\n",
    "# alpha_wrong = tf.reduce_sum(tf.multiply(attention,sentences[story_length+1]),axis=1)\n",
    "print(alpha_correct.shape)\n",
    "\n",
    "alpha_correct = tf.transpose(tf.reshape(alpha_correct,[seq_len,batch_size,story_length]),perm=[1,2,0])\n",
    "alpha_wrong = tf.transpose(tf.reshape(alpha_wrong,[seq_len,batch_size,story_length]),perm=[1,2,0])\n",
    "\n",
    "# alpha_correct: batch_size x story_length x seq_len \n",
    "print(alpha_correct.shape)\n",
    "\n",
    "# beta_correct: batch_size x story_length x seq_len\n",
    "beta_correct = tf.nn.softmax(alpha_correct) \n",
    "beta_wrong = tf.nn.softmax(alpha_wrong) \n",
    "\n",
    "beta_correct = tf.transpose(tf.reshape(beta_correct,[batch_size*story_length,seq_len]))\n",
    "beta_wrong = tf.transpose(tf.reshape(beta_wrong,[batch_size*story_length,seq_len]))\n",
    "print(beta_correct.shape)\n",
    "\n",
    "# beta_correct: seq_len x batch_size*story_length\n",
    "# h_layer: seq_len x batch_size*(story_length+2) x num_units\n",
    "sentences_atten_correct = tf.multiply(tf.reshape(h_layer[:,:,:story_length,:],[-1,num_units]),tf.reshape(beta_correct,[-1,1]))\n",
    "sentences_atten_correct = tf.reduce_sum(tf.reshape(sentences_atten_correct,[seq_len,batch_size*story_length,-1]),axis=0)\n",
    "\n",
    "sentences_atten_wrong = tf.multiply(tf.reshape(h_layer[:,:,:story_length,:],[-1,num_units]),tf.reshape(beta_wrong,[-1,1]))\n",
    "sentences_atten_wrong = tf.reduce_sum(tf.reshape(sentences_atten_wrong,[seq_len,batch_size*story_length,-1]),axis=0)\n",
    "\n",
    "print(sentences_atten_correct.shape)\n",
    "# print(tf.multiply(tf.reshape(h_layer[:,:,:story_length,:],[-1,num_units]),tf.reshape(beta_correct,[-1,1])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 300)\n",
      "(100, 300)\n",
      "(50, 4, 300)\n",
      "(100, 4, 300)\n",
      "4\n",
      "(100, 300)\n"
     ]
    }
   ],
   "source": [
    "##### Embed Stories #####\n",
    "# correct and wrong sentences are putted together respectively\n",
    "\n",
    "sentences_atten_correct = tf.reshape(sentences_atten_correct,[batch_size,story_length,num_units])\n",
    "sentences_atten_wrong = tf.reshape(sentences_atten_wrong,[batch_size,story_length,num_units])\n",
    "\n",
    "endings = tf.reshape(sentences,[batch_size,story_length+2,num_units])[:,story_length:,:]\n",
    "print(endings[:,0,:].shape)\n",
    "endings = tf.concat([endings[:,0,:], endings[:,1,:]],axis=0)\n",
    "print(endings.shape)\n",
    "endings = tf.reshape(endings,[batch_size*2,-1])\n",
    "\n",
    "lstm_forward2 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "lstm_backward2 = tf.nn.rnn_cell.LSTMCell(num_units=num_units,\n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "state_forward2 = lstm_forward2.zero_state(batch_size*2, 'float')\n",
    "state_backward2 = lstm_backward2.zero_state(batch_size*2, 'float')\n",
    "\n",
    "states_forward2 = []\n",
    "states_backward2 = []\n",
    "\n",
    "print(sentences_atten_correct.shape)\n",
    "\n",
    "sentence_att = tf.concat([sentences_atten_correct,sentences_atten_wrong],axis=0)\n",
    "\n",
    "\n",
    "print(sentence_att.shape)\n",
    "sentence_att_unstack = tf.unstack(sentence_att,story_length,1)\n",
    "\n",
    "print(len(sentence_att_unstack))\n",
    "\n",
    "for t in range(story_length):\n",
    "    \n",
    "\tp, state_forward2 = lstm_forward2(sentence_att_unstack[t], state_forward2)\n",
    "\tstates_forward2.append(state_forward2.h)\n",
    "\n",
    "\tp, state_backward2 = lstm_backward2(sentence_att_unstack[story_length-1-t], state_backward2)\n",
    "\tstates_backward2.append(state_backward2.h)\n",
    "\n",
    "# states_backward2.reverse()\n",
    "\n",
    "state_forward2 = lstm_forward2.zero_state(batch_size*2, 'float')\n",
    "state_backward2 = lstm_backward2.zero_state(batch_size*2, 'float')\n",
    "\n",
    "p, endings_forward = lstm_forward2(endings, state_forward2)\n",
    "p, endings_backward = lstm_backward2(endings, state_backward2)\n",
    "\n",
    "stories = tf.add(states_forward2[-1],states_backward2[-1])\n",
    "endings_embed = tf.add(endings_forward.h,endings_backward.h)\n",
    "\n",
    "print(stories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 600)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "##### Calculate Score, loss, and do updating #####\n",
    "hier_stories = tf.concat([stories,endings_embed],axis=1)\n",
    "print(hier_stories.shape)\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale)\n",
    "hidden_layer = tf.layers.dense(hier_stories, 512, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                               activation=tf.nn.relu,kernel_regularizer=regularizer)\n",
    "\n",
    "scores = tf.layers.dense(hidden_layer, 1, kernel_regularizer=regularizer,#activation=tf.nn.tanh,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "# logits = tf.layers.dense(\n",
    "#     states, num_vocab, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "losses = tf.maximum(tf.add(tf.subtract(scores[batch_size:],scores[:batch_size]),1),0)\n",
    "print(losses.shape)\n",
    "loss = tf.reduce_sum(losses)\n",
    "\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "loss += l2_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()  # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 6, 30)\n",
      "(1871, 6, 30)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_ensem/model_biLSTM_att_val_ensem_0.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./saved_ensem/model_biLSTM_att_val_ensem_1.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./saved_ensem/model_biLSTM_att_val_ensem_2.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./saved_ensem/model_biLSTM_att_val_ensem_3.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./saved_ensem/model_biLSTM_att_val_ensem_4.ckpt\n",
      "epoch_test: loss: 3.805042988545186 accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# Train the model\n",
    "# ----------------------------------------------\n",
    "epoch = 10\n",
    "fold_num = 5\n",
    "load_saved = True\n",
    "\n",
    "# X_Train.shape: (88161, 6, 19)\n",
    "print(X_ori.shape)\n",
    "\n",
    "single_fold = X_train.shape[0]//10\n",
    "\n",
    "###### save the last tenth for validation #####\n",
    "X_train = np.concatenate((X_train[:1*single_fold],X_train[2*single_fold:])) \n",
    "single_fold = X_ori.shape[0]//10\n",
    "X_train = X_ori[:9*single_fold] \n",
    "X_val = X_ori[9*single_fold:] \n",
    "print(X_test.shape)\n",
    "\n",
    "# X_train = X_ori\n",
    "############################################\n",
    "\n",
    "np.random.shuffle(X_ori)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    single_fold = X_train.shape[0]//fold_num\n",
    "    \n",
    "    if not load_saved:\n",
    "        for k in range(fold_num):         \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "#             X_train_temp = np.concatenate((X_train[:k*single_fold],X_train[(k+1)*single_fold:])) \n",
    "            X_train_temp =  X_train\n",
    "\n",
    "#             X_val = X_train[k*single_fold:(k+1)*single_fold]\n",
    "\n",
    "            for i in range(epoch):\n",
    "                accuracy = 0\n",
    "                loss_mean = 0\n",
    "                for j in range(X_train.shape[0]//batch_size):\n",
    "\n",
    "                    X_batch = X_train_temp[np.random.choice(X_train_temp.shape[0],batch_size)]\n",
    "\n",
    "                    X_batch_embed = np.zeros((batch_size,(story_length+2), seq_len, embedding_dim))\n",
    "                    X_batch_embed = embedding_matrix[X_batch]\n",
    "                    X_batch_embed = np.reshape(X_batch_embed,(batch_size*(story_length+2), seq_len, embedding_dim))\n",
    "\n",
    "                    score, loss_sofar, _ = sess.run([scores,loss,train_step],feed_dict = {embedding: X_batch_embed})\n",
    "                    accuracy += np.mean((score[:batch_size]-score[batch_size:])>0)\n",
    "                    loss_mean += loss_sofar\n",
    "        #             print(score.reshape([-1])[:batch_size])\n",
    "        #             print(score.reshape([-1])[batch_size:])\n",
    "\n",
    "                    if(j % 10 == 0):\n",
    "                        print(\"epoch:\",i,j, X_train.shape[0]//batch_size,\"loss:\",loss_mean/(j+1),\"accuracy:\",accuracy/(j+1))\n",
    "\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, './saved_ensem/model_biLSTM_att_val_ensem_'+str(k)+'.ckpt')\n",
    "\n",
    "                # validation\n",
    "                accuracy = 0\n",
    "                loss_mean = 0\n",
    "                \n",
    "                for j in range(X_val.shape[0]//batch_size):\n",
    "                    X_val_temp = X_val[j*batch_size:(j+1)*batch_size]\n",
    "                    X_val_embed = embedding_matrix[X_val_temp]\n",
    "\n",
    "                    X_val_embed = np.reshape(X_val_embed,(batch_size*(story_length+2), seq_len, embedding_dim))\n",
    "\n",
    "\n",
    "                    score, loss_sofar = sess.run([scores,loss],feed_dict = {embedding: X_val_embed})\n",
    "                    accuracy += np.mean((score[:batch_size]-score[batch_size:])>0)\n",
    "                    loss_mean += loss_sofar\n",
    "                \n",
    "#                 print(X_val.shape[0]//batch_size)\n",
    "                accuracy /= (X_val.shape[0]//batch_size)\n",
    "                loss_mean /= (X_val.shape[0]//batch_size)\n",
    "                print(\"epoch_val:\",i, \"loss:\",loss_mean,\"accuracy:\",accuracy)\n",
    "           \n",
    "    else: \n",
    "        \n",
    "#         result = np.zeros(((X_test.shape[0]//batch_size)*batch_size,fold_num),dtype = np.int32)\n",
    "        result = np.zeros(((X_val.shape[0]//batch_size)*batch_size,fold_num),dtype = np.int32)\n",
    "            \n",
    "        for k in range(fold_num): \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, './saved_ensem/model_biLSTM_att_val_ensem_'+str(k)+'.ckpt')\n",
    "            \n",
    "            accuracy = 0\n",
    "            loss_mean = 0\n",
    "            \n",
    "#             for j in range(X_test.shape[0]//batch_size):\n",
    "            for j in range(X_val.shape[0]//batch_size):\n",
    "                X_test_temp = X_val[j*batch_size:(j+1)*batch_size]\n",
    "                X_test_embed = embedding_matrix[X_test_temp]\n",
    "\n",
    "                X_test_embed = np.reshape(X_test_embed,(batch_size*(story_length+2), seq_len, embedding_dim))\n",
    "\n",
    "                score, loss_sofar = sess.run([scores,loss],feed_dict = {embedding: X_test_embed})\n",
    "                score = np.concatenate((score[:batch_size], score[batch_size:]),axis=1)\n",
    "                \n",
    "                result[j*batch_size:(j+1)*batch_size,k] = np.argmax(score,axis=1)\n",
    "#                 print(result[j*batch_size:(j+1)*batch_size,k])\n",
    "                loss_mean += loss_sofar\n",
    "                \n",
    "#                 accuracy += np.mean((score[:batch_size]-score[batch_size:])>0)\n",
    "        \n",
    "        scores = np.sum(result,axis=1)\n",
    "        accuracy = np.mean(scores < 3)\n",
    "        \n",
    "#         accuracy /= (X_test.shape[0]//batch_size)\n",
    "        loss_mean /= (X_test.shape[0]//batch_size)\n",
    "        print(\"epoch_test:\", \"loss:\",loss_mean,\"accuracy:\",accuracy)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
